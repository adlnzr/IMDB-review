{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
     ]
    }
   ],
   "source": [
    "!cat aclImdb/train/pos/4077_10.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(\"aclImdb/train/neg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1821_4.txt'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"aclImdb/train/neg\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'aclImdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_(data_file):\n",
    "    with open (os.path.join(data_file), 'r') as f:\n",
    "        data = f.read()\n",
    "        return data\n",
    "\n",
    "pos_dataset_ = []\n",
    "neg_dataset_ = []\n",
    "\n",
    "for data_file in os.listdir(os.path.join(DATA_DIR, 'train', 'pos')):\n",
    "    data_path = os.path.join(DATA_DIR, 'train', 'pos', data_file)\n",
    "    pos_dataset_.append(make_dataset_(data_path))\n",
    "\n",
    "for data_file in os.listdir(os.path.join(DATA_DIR, 'train', 'neg')):\n",
    "    data_path = os.path.join(DATA_DIR, 'train', 'neg', data_file)\n",
    "    neg_dataset_.append(make_dataset_(data_path))\n",
    "\n",
    "dataset_ = pos_dataset_ + neg_dataset_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500\n",
      "12500\n"
     ]
    }
   ],
   "source": [
    "print(len(pos_dataset_))\n",
    "print(len(neg_dataset_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. Imagine a movie where Joe Piscopo is actually funny! Maureen Stapleton is a scene stealer. The Moroni character is an absolute scream. Watch for Alan \"The Skipper\" Hale jr. as a police Sgt.'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_dataset_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(text):\n",
    "    text = text.lower()\n",
    "    return \"\".join(char for char in text if char\n",
    "                   not in string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = standardize(text)\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'book022'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardize('BOok#022')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['book02', 'is', 'something', 'good']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize('BOok#02 Is something good?!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary():\n",
    "    def __init__(self, freq=3):\n",
    "\n",
    "        self.stoi = {'<pad>':0 ,'<SOS>':1, '<EOS>':2, '<UNK>':3}\n",
    "        self.itos = {0:'<pad>', 1:'<SOS>', 2:'<EOS>', 3:'<UNK>'}\n",
    "\n",
    "        self.freq = freq\n",
    "\n",
    "    def standardize(text):\n",
    "        text = text.lower()\n",
    "        return \"\".join(char for char in text if\n",
    "                    char not in string.punctuation)\n",
    "    \n",
    "    def tokenize(text):\n",
    "            text = standardize(text)\n",
    "            return text.split()\n",
    "\n",
    "    def make_tokens(self, dataset):\n",
    "        temp_vocab = {}\n",
    "\n",
    "        for text in dataset:\n",
    "            tokens = tokenize(text)\n",
    "\n",
    "            for token in tokens:\n",
    "                if token not in temp_vocab:\n",
    "                    temp_vocab[token] = 1\n",
    "                else:\n",
    "                    temp_vocab[token] +=1\n",
    "\n",
    "                if temp_vocab[token] == self.freq:\n",
    "                    index = len(self.stoi)\n",
    "                    self.stoi[token] = index\n",
    "                    self.itos[index] = token\n",
    "            \n",
    "    def encode(self, text):\n",
    "        tokens = tokenize(text)\n",
    "        return ([self.stoi['<SOS>']] + [self.stoi.get(token, 3) for token in tokens]\n",
    "                 + [self.stoi['<EOS>']])\n",
    "\n",
    "    def decode(self, int_seq):\n",
    "        return \" \".join(self.itos.get(int, 'UNK') for int in int_seq)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_ = Vocabulary(3)\n",
    "tokenizer_.make_tokens(dataset_)\n",
    "encoded_ = tokenizer_.encode(dataset_[0])\n",
    "decoded_ = tokenizer_.decode(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<SOS> for a movie that gets no respect there sure are a lot of memorable quotes listed for this gem imagine a movie where joe piscopo is actually funny maureen stapleton is a scene stealer the moroni character is an absolute scream watch for alan the skipper hale jr as a police sgt <EOS>'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(decoded_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(data_dir : str, tokenizer: Vocabulary) -> Vocabulary:\n",
    "\n",
    "    dataset = []\n",
    "    path_list = []\n",
    "\n",
    "    path_ = os.path.join(data_dir, 'train')\n",
    "    path_list = path_list = (glob(os.path.join(path_, 'pos', '*.txt')) +\n",
    "                     glob(os.path.join(path_, 'neg', '*.txt')))\n",
    "\n",
    "    for data_path in path_list:\n",
    "        with open(data_path, 'r') as f:\n",
    "            text = f.read()\n",
    "            dataset.append(text)\n",
    "\n",
    "    tokenizer.make_tokens(dataset)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, data_dir, tokenizer=Vocabulary, train=True, random_state=42):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self.nlp = tokenizer\n",
    "        self.data = []\n",
    "\n",
    "        if train:\n",
    "            path = os.path.join(data_dir, 'train')\n",
    "        else:\n",
    "            path = os.path.join(data_dir, 'test')\n",
    "        \n",
    "        for label in ['pos', 'neg']:\n",
    "            final_path = os.path.join(path, label)\n",
    "            for i in glob(final_path + '/*.txt'):\n",
    "                self.data.append((i, label == 'pos'))\n",
    "\n",
    "        random.Random(random_state).shuffle(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        file_path, label = self.data[idx]\n",
    "        \n",
    "        with open(file_path, 'r') as f:\n",
    "            text_data = f.read()\n",
    "            data = self.nlp.encode(text_data)\n",
    "        return torch.tensor(data).long(), torch.tensor([label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'aclImdb'\n",
    "\n",
    "tokenizer = Vocabulary(freq = 3)\n",
    "\n",
    "nlp = build_vocab(DATA_DIR, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '<SOS>': 1,\n",
       " '<EOS>': 2,\n",
       " '<UNK>': 3,\n",
       " 'a': 4,\n",
       " 'is': 5,\n",
       " 'for': 6,\n",
       " 'movie': 7,\n",
       " 'the': 8,\n",
       " 'with': 9,\n",
       " 'as': 10,\n",
       " 'but': 11,\n",
       " 'raines': 12,\n",
       " 'by': 13,\n",
       " 'are': 14,\n",
       " 'of': 15,\n",
       " 'and': 16,\n",
       " 'to': 17,\n",
       " 'i': 18,\n",
       " 'was': 19,\n",
       " 'who': 20,\n",
       " 'that': 21,\n",
       " 'ring': 22,\n",
       " 'br': 23,\n",
       " 'joe': 24,\n",
       " 'his': 25,\n",
       " 'billy': 26,\n",
       " 'worms': 27,\n",
       " 'he': 28,\n",
       " 'in': 29,\n",
       " 'on': 30,\n",
       " 'like': 31,\n",
       " 'kids': 32,\n",
       " 'if': 33,\n",
       " 'an': 34,\n",
       " 'at': 35,\n",
       " 'fried': 36,\n",
       " 'it': 37,\n",
       " 'has': 38,\n",
       " 'you': 39,\n",
       " 'this': 40,\n",
       " 'their': 41,\n",
       " 'only': 42,\n",
       " 'film': 43,\n",
       " 'when': 44,\n",
       " 'me': 45,\n",
       " 'great': 46,\n",
       " 'have': 47,\n",
       " 'can': 48,\n",
       " 'lot': 49,\n",
       " 'my': 50,\n",
       " 'all': 51,\n",
       " 'wonderful': 52,\n",
       " 'show': 53,\n",
       " 'its': 54,\n",
       " 'about': 55,\n",
       " 'no': 56,\n",
       " 'way': 57,\n",
       " 'out': 58,\n",
       " 'two': 59,\n",
       " 'there': 60,\n",
       " 'not': 61,\n",
       " 'so': 62,\n",
       " 'know': 63,\n",
       " 'children': 64,\n",
       " 'also': 65,\n",
       " 'death': 66,\n",
       " 'liked': 67,\n",
       " 'some': 68,\n",
       " 'be': 69,\n",
       " 'given': 70,\n",
       " 'or': 71,\n",
       " 'how': 72,\n",
       " 'ever': 73,\n",
       " 'were': 74,\n",
       " 'any': 75,\n",
       " 'again': 76,\n",
       " 'see': 77,\n",
       " 'loved': 78,\n",
       " 'brought': 79,\n",
       " 'made': 80,\n",
       " 'good': 81,\n",
       " 'verger': 82,\n",
       " 'new': 83,\n",
       " 'hayter': 84,\n",
       " 'from': 85,\n",
       " 'mr': 86,\n",
       " 'max': 87,\n",
       " 'things': 88,\n",
       " 'soon': 89,\n",
       " 'story': 90,\n",
       " 'jewelry': 91,\n",
       " 'real': 92,\n",
       " 'maugham': 93,\n",
       " 'one': 94,\n",
       " 'time': 95,\n",
       " 'sanitorium': 96,\n",
       " 'people': 97,\n",
       " 'michael': 98,\n",
       " 'stories': 99,\n",
       " 'well': 100,\n",
       " 'man': 101,\n",
       " 'over': 102,\n",
       " 'mature': 103,\n",
       " 'life': 104,\n",
       " 'never': 105,\n",
       " 'after': 106,\n",
       " 'such': 107,\n",
       " 'characters': 108,\n",
       " 'scene': 109,\n",
       " 'more': 110,\n",
       " 'her': 111,\n",
       " 'say': 112,\n",
       " 'favorite': 113,\n",
       " 'lovely': 114,\n",
       " 'although': 115,\n",
       " 'does': 116,\n",
       " 'piece': 117,\n",
       " 'they': 118,\n",
       " 'into': 119,\n",
       " 'face': 120,\n",
       " 'use': 121,\n",
       " '70s': 122,\n",
       " 'where': 123,\n",
       " 'serious': 124,\n",
       " 'what': 125,\n",
       " 'role': 126,\n",
       " 'watch': 127,\n",
       " 'first': 128,\n",
       " 'would': 129,\n",
       " 'very': 130,\n",
       " 'actually': 131,\n",
       " 'myself': 132,\n",
       " 'then': 133,\n",
       " 'which': 134,\n",
       " 'could': 135,\n",
       " 'cinematography': 136,\n",
       " 'here': 137,\n",
       " 'makes': 138,\n",
       " 'something': 139,\n",
       " 'thing': 140,\n",
       " 'your': 141,\n",
       " 'director': 142,\n",
       " 'had': 143,\n",
       " 'john': 144,\n",
       " 'because': 145,\n",
       " 'do': 146,\n",
       " 'cain': 147,\n",
       " 'remarkable': 148,\n",
       " 'even': 149,\n",
       " 'playing': 150,\n",
       " 'actors': 151,\n",
       " 'involved': 152,\n",
       " 'nearly': 153,\n",
       " 'much': 154,\n",
       " 'stone': 155,\n",
       " 'boy': 156,\n",
       " 'theres': 157,\n",
       " 'talk': 158,\n",
       " 'quality': 159,\n",
       " 'saw': 160,\n",
       " 'world': 161,\n",
       " 'feeling': 162,\n",
       " '1983': 163,\n",
       " 'secret': 164,\n",
       " 'day': 165,\n",
       " 'genius': 166,\n",
       " 'young': 167,\n",
       " 'strange': 168,\n",
       " 'up': 169,\n",
       " 'while': 170,\n",
       " 'she': 171,\n",
       " 'career': 172,\n",
       " 'release': 173,\n",
       " 'through': 174,\n",
       " 'drugs': 175,\n",
       " 'still': 176,\n",
       " 'funny': 177,\n",
       " 'get': 178,\n",
       " 'human': 179,\n",
       " 'police': 180,\n",
       " 'am': 181,\n",
       " 'now': 182,\n",
       " 'should': 183,\n",
       " 'old': 184,\n",
       " 'read': 185,\n",
       " 'books': 186,\n",
       " 'nancy': 187,\n",
       " 'them': 188,\n",
       " 'perfect': 189,\n",
       " 'car': 190,\n",
       " 'go': 191,\n",
       " 'been': 192,\n",
       " 'really': 193,\n",
       " 'films': 194,\n",
       " 'exactly': 195,\n",
       " 'beautiful': 196,\n",
       " 'than': 197,\n",
       " 'bit': 198,\n",
       " 'we': 199,\n",
       " 'think': 200,\n",
       " 'character': 201,\n",
       " 'must': 202,\n",
       " 'end': 203,\n",
       " 'im': 204,\n",
       " 'just': 205,\n",
       " 'harris': 206,\n",
       " 'lennon': 207,\n",
       " 'love': 208,\n",
       " 'america': 209,\n",
       " 'far': 210,\n",
       " 'dvd': 211,\n",
       " 'episodes': 212,\n",
       " 'each': 213,\n",
       " 'other': 214,\n",
       " 'shows': 215,\n",
       " 'joke': 216,\n",
       " 'every': 217,\n",
       " 'make': 218,\n",
       " 'another': 219,\n",
       " 'space': 220,\n",
       " 'best': 221,\n",
       " 'little': 222,\n",
       " 'terms': 223,\n",
       " 'being': 224,\n",
       " 'gets': 225,\n",
       " 'own': 226,\n",
       " 'dog': 227,\n",
       " 'big': 228,\n",
       " 'yeti': 229,\n",
       " 'stuff': 230,\n",
       " 'look': 231,\n",
       " 'giant': 232,\n",
       " 'shot': 233,\n",
       " 'upon': 234,\n",
       " 'most': 235,\n",
       " 'work': 236,\n",
       " 'same': 237,\n",
       " 'felt': 238,\n",
       " 'those': 239,\n",
       " 'rather': 240,\n",
       " 'part': 241,\n",
       " 'fun': 242,\n",
       " 'let': 243,\n",
       " 'why': 244,\n",
       " 'id': 245,\n",
       " 'monster': 246,\n",
       " 'movies': 247,\n",
       " 'fake': 248,\n",
       " 'full': 249,\n",
       " 'adventure': 250,\n",
       " 'small': 251,\n",
       " 'will': 252,\n",
       " 'stupid': 253,\n",
       " 'parents': 254,\n",
       " 'ten': 255,\n",
       " 'too': 256,\n",
       " 'long': 257,\n",
       " 'once': 258,\n",
       " 'worth': 259,\n",
       " 'didnt': 260,\n",
       " 'behind': 261,\n",
       " 'fashion': 262,\n",
       " 'years': 263,\n",
       " 'message': 264,\n",
       " 'tells': 265,\n",
       " 'both': 266,\n",
       " 'beautifully': 267,\n",
       " 'quite': 268,\n",
       " 'touching': 269,\n",
       " 'boys': 270,\n",
       " 'easy': 271,\n",
       " 'off': 272,\n",
       " 'may': 273,\n",
       " 'might': 274,\n",
       " 'probably': 275,\n",
       " 'live': 276,\n",
       " 'tv': 277,\n",
       " 'before': 278,\n",
       " 'him': 279,\n",
       " 'name': 280,\n",
       " 'someone': 281,\n",
       " 'stops': 282,\n",
       " 'ben': 283,\n",
       " 'yoda': 284,\n",
       " 'ending': 285,\n",
       " 'interesting': 286,\n",
       " 'spirit': 287,\n",
       " 'back': 288,\n",
       " 'thought': 289,\n",
       " 'station': 290,\n",
       " 'plays': 291,\n",
       " 'guy': 292,\n",
       " 'learn': 293,\n",
       " 'nothing': 294,\n",
       " 'seen': 295,\n",
       " 'enjoy': 296,\n",
       " 'highly': 297,\n",
       " 'recommend': 298,\n",
       " 'living': 299,\n",
       " 'opening': 300,\n",
       " 'dead': 301,\n",
       " 'many': 302,\n",
       " 'course': 303,\n",
       " 'cast': 304,\n",
       " 'hood': 305,\n",
       " 'holds': 306,\n",
       " 'together': 307,\n",
       " 'moving': 308,\n",
       " 'brother': 309,\n",
       " 'zombie': 310,\n",
       " 'eat': 311,\n",
       " 'men': 312,\n",
       " 'everyone': 313,\n",
       " 'horror': 314,\n",
       " 'thats': 315,\n",
       " 'result': 316,\n",
       " 'follow': 317,\n",
       " 'ive': 318,\n",
       " 'driveby': 319,\n",
       " 'zombies': 320,\n",
       " 'dont': 321,\n",
       " 'shots': 322,\n",
       " 'these': 323,\n",
       " 'cool': 324,\n",
       " 'james': 325,\n",
       " 'having': 326,\n",
       " 'series': 327,\n",
       " 'excellent': 328,\n",
       " 'acting': 329,\n",
       " 'top': 330,\n",
       " 'better': 331,\n",
       " 'talented': 332,\n",
       " 'father': 333,\n",
       " 'anything': 334,\n",
       " 'however': 335,\n",
       " 'wife': 336,\n",
       " 'whether': 337,\n",
       " 'money': 338,\n",
       " 'star': 339,\n",
       " 'played': 340,\n",
       " 'performance': 341,\n",
       " 'alone': 342,\n",
       " 'already': 343,\n",
       " 'got': 344,\n",
       " 'almost': 345,\n",
       " 'pilot': 346,\n",
       " 'hope': 347,\n",
       " 'visual': 348,\n",
       " 'effects': 349,\n",
       " 'havent': 350,\n",
       " 'bsg': 351,\n",
       " 'ill': 352,\n",
       " 'fan': 353,\n",
       " 'expressions': 354,\n",
       " 'delivers': 355,\n",
       " 'based': 356,\n",
       " 'roles': 357,\n",
       " 'himself': 358,\n",
       " 'business': 359,\n",
       " 'sullivan': 360,\n",
       " 'keep': 361,\n",
       " 'comes': 362,\n",
       " 'direction': 363,\n",
       " 'dark': 364,\n",
       " 'driving': 365,\n",
       " 'mike': 366,\n",
       " 'front': 367,\n",
       " 'around': 368,\n",
       " 'side': 369,\n",
       " 'completely': 370,\n",
       " 'gorgeous': 371,\n",
       " 'debut': 372,\n",
       " 'beauty': 373,\n",
       " 'worked': 374,\n",
       " 'play': 375,\n",
       " 'theater': 376,\n",
       " 'mendes': 377,\n",
       " 'true': 378,\n",
       " 'waiting': 379,\n",
       " 'watching': 380,\n",
       " 'did': 381,\n",
       " 'laugh': 382,\n",
       " 'watched': 383,\n",
       " 'heart': 384,\n",
       " 'quiet': 385,\n",
       " 'tom': 386,\n",
       " 'seriously': 387,\n",
       " 'special': 388,\n",
       " 'doesnt': 389,\n",
       " 'happen': 390,\n",
       " 'kind': 391,\n",
       " 'sure': 392,\n",
       " 'take': 393,\n",
       " 'actual': 394,\n",
       " 'hell': 395,\n",
       " 'today': 396,\n",
       " 'reality': 397,\n",
       " 'future': 398,\n",
       " 'classic': 399,\n",
       " 'saying': 400,\n",
       " 'hard': 401,\n",
       " 'usual': 402,\n",
       " 'won': 403,\n",
       " 'hero': 404,\n",
       " 'stolen': 405,\n",
       " 'bank': 406,\n",
       " 'early': 407,\n",
       " 'indian': 408,\n",
       " 'rifle': 409,\n",
       " 'western': 410,\n",
       " 'action': 411,\n",
       " 'paul': 412,\n",
       " 'peter': 413,\n",
       " 'though': 414,\n",
       " 'done': 415,\n",
       " 'couldnt': 416,\n",
       " 'idea': 417,\n",
       " 'going': 418,\n",
       " 'came': 419,\n",
       " 'sometimes': 420,\n",
       " 'down': 421,\n",
       " 'humanity': 422,\n",
       " 'strong': 423,\n",
       " 'pure': 424,\n",
       " 'without': 425,\n",
       " 'female': 426,\n",
       " 'job': 427,\n",
       " 'powerful': 428,\n",
       " 'met': 429,\n",
       " 'fans': 430,\n",
       " 'nemesis': 431,\n",
       " '2': 432,\n",
       " 'point': 433,\n",
       " 'bodybuilder': 434,\n",
       " 'us': 435,\n",
       " 'enjoyed': 436,\n",
       " 'enthusiasm': 437,\n",
       " '3': 438,\n",
       " 'shown': 439,\n",
       " 'night': 440,\n",
       " 'admit': 441,\n",
       " 'instead': 442,\n",
       " 'music': 443,\n",
       " 'terrific': 444,\n",
       " 'family': 445,\n",
       " 'edie': 446,\n",
       " 'famous': 447,\n",
       " 'kong': 448,\n",
       " 'surprisingly': 449,\n",
       " 'our': 450,\n",
       " 'modern': 451,\n",
       " 'wants': 452,\n",
       " 'crime': 453,\n",
       " 'high': 454,\n",
       " 'eva': 455,\n",
       " 'nick': 456,\n",
       " 'come': 457,\n",
       " 'want': 458,\n",
       " 'sequel': 459,\n",
       " 'perhaps': 460,\n",
       " 'slow': 461,\n",
       " 'gives': 462,\n",
       " 'disease': 463,\n",
       " 'viewing': 464,\n",
       " 'believable': 465,\n",
       " 'anyone': 466,\n",
       " 'last': 467,\n",
       " 'charlotte': 468,\n",
       " 'novel': 469,\n",
       " 'remember': 470,\n",
       " 'natural': 471,\n",
       " 'compare': 472,\n",
       " 'laura': 473,\n",
       " 'find': 474,\n",
       " 'ends': 475,\n",
       " 'seeing': 476,\n",
       " 'sidewalk': 477,\n",
       " 'getting': 478,\n",
       " 'body': 479,\n",
       " 'performances': 480,\n",
       " 'cop': 481,\n",
       " 'local': 482,\n",
       " 'goes': 483,\n",
       " 'number': 484,\n",
       " 'murder': 485,\n",
       " 'means': 486,\n",
       " 'andrews': 487,\n",
       " 'always': 488,\n",
       " 'delightful': 489,\n",
       " 'tierney': 490,\n",
       " 'seem': 491,\n",
       " 'solid': 492,\n",
       " 'sets': 493,\n",
       " 'filmbr': 494,\n",
       " 'believe': 495,\n",
       " 'seems': 496,\n",
       " 'nature': 497,\n",
       " 'portrayal': 498,\n",
       " 'spooky': 499,\n",
       " 'white': 500,\n",
       " 'superb': 501,\n",
       " 'surprise': 502,\n",
       " 'totally': 503,\n",
       " 'set': 504,\n",
       " 'alan': 505,\n",
       " 'during': 506,\n",
       " 'poe': 507,\n",
       " 'bet': 508,\n",
       " 'castle': 509,\n",
       " 'cannot': 510,\n",
       " '10': 511,\n",
       " 'black': 512,\n",
       " 'aka': 513,\n",
       " 'blood': 514,\n",
       " 'making': 515,\n",
       " 'moments': 516,\n",
       " 'violence': 517,\n",
       " 'especially': 518,\n",
       " 'italian': 519,\n",
       " 'pity': 520,\n",
       " 'found': 521,\n",
       " 'year': 522,\n",
       " 'took': 523,\n",
       " 'age': 524,\n",
       " 'town': 525,\n",
       " 'else': 526,\n",
       " 'scenes': 527,\n",
       " 'genre': 528,\n",
       " 'fictional': 529,\n",
       " 'killed': 530,\n",
       " 'starts': 531,\n",
       " 'whole': 532,\n",
       " 'between': 533,\n",
       " 'wonder': 534,\n",
       " 'writer': 535,\n",
       " 'harry': 536,\n",
       " 'budget': 537,\n",
       " 'looking': 538,\n",
       " 'different': 539,\n",
       " 'appearance': 540,\n",
       " 'interest': 541,\n",
       " 'plot': 542,\n",
       " 'knowing': 543,\n",
       " 'second': 544,\n",
       " 'couple': 545,\n",
       " 'days': 546,\n",
       " 'put': 547,\n",
       " 'riding': 548,\n",
       " 'giants': 549,\n",
       " 'experience': 550,\n",
       " 'beyond': 551,\n",
       " 'lived': 552,\n",
       " 'tell': 553,\n",
       " 'water': 554,\n",
       " 'waves': 555,\n",
       " 'away': 556,\n",
       " 'crew': 557,\n",
       " 'dies': 558,\n",
       " 'brothers': 559,\n",
       " 'person': 560,\n",
       " 'actor': 561,\n",
       " 'chance': 562,\n",
       " 'near': 563,\n",
       " 'child': 564,\n",
       " 'sense': 565,\n",
       " 'mans': 566,\n",
       " 'cheap': 567,\n",
       " 'sound': 568,\n",
       " 'bad': 569,\n",
       " 'few': 570,\n",
       " 'give': 571,\n",
       " 'try': 572,\n",
       " 'hit': 573,\n",
       " 'perfectly': 574,\n",
       " 'start': 575,\n",
       " 'singing': 576,\n",
       " 'doodlebops': 577,\n",
       " 'song': 578,\n",
       " 'childrens': 579,\n",
       " 'meet': 580,\n",
       " 'absolutely': 581,\n",
       " 'amazed': 582,\n",
       " 'problems': 583,\n",
       " 'talent': 584,\n",
       " 'amazing': 585,\n",
       " 'festival': 586,\n",
       " 'went': 587,\n",
       " 'truly': 588,\n",
       " 'takes': 589,\n",
       " 'gave': 590,\n",
       " 'cars': 591,\n",
       " 'friends': 592,\n",
       " 'bro': 593,\n",
       " 'times': 594,\n",
       " 'fancy': 595,\n",
       " 'since': 596,\n",
       " '50': 597,\n",
       " 'hot': 598,\n",
       " 'pretty': 599,\n",
       " 'ok': 600,\n",
       " 'woman': 601,\n",
       " 'yourself': 602,\n",
       " 'entertaining': 603,\n",
       " 'chases': 604,\n",
       " 'wild': 605,\n",
       " 'sexy': 606,\n",
       " 'lady': 607,\n",
       " 'least': 608,\n",
       " 'nice': 609,\n",
       " 'overall': 610,\n",
       " 'favourite': 611,\n",
       " 'eyes': 612,\n",
       " 'maybe': 613,\n",
       " 'rock': 614,\n",
       " 'clearly': 615,\n",
       " 'under': 616,\n",
       " 'bruckheimer': 617,\n",
       " 'worse': 618,\n",
       " 'cage': 619,\n",
       " 'successful': 620,\n",
       " 'cages': 621,\n",
       " 'seemed': 622,\n",
       " 'latter': 623,\n",
       " 'yet': 624,\n",
       " 'three': 625,\n",
       " 'yes': 626,\n",
       " 'said': 627,\n",
       " 'con': 628,\n",
       " 'air': 629,\n",
       " 'act': 630,\n",
       " 'fast': 631,\n",
       " 'able': 632,\n",
       " 'jolie': 633,\n",
       " 'certainly': 634,\n",
       " '7': 635,\n",
       " 'planet': 636,\n",
       " 'isnt': 637,\n",
       " 'figure': 638,\n",
       " 'tears': 639,\n",
       " 'offers': 640,\n",
       " 'emotional': 641,\n",
       " 'daughter': 642,\n",
       " 'close': 643,\n",
       " 'patrick': 644,\n",
       " 'turned': 645,\n",
       " 'brilliant': 646,\n",
       " 'feel': 647,\n",
       " 'writing': 648,\n",
       " 'evening': 649,\n",
       " 'above': 650,\n",
       " 'american': 651,\n",
       " 'mothers': 652,\n",
       " 'across': 653,\n",
       " 'stars': 654,\n",
       " 'redgrave': 655,\n",
       " 'richardson': 656,\n",
       " 'gummer': 657,\n",
       " 'exciting': 658,\n",
       " 'actress': 659,\n",
       " 'price': 660,\n",
       " 'sit': 661,\n",
       " 'cinema': 662,\n",
       " 'hours': 663,\n",
       " 'explanation': 664,\n",
       " 'television': 665,\n",
       " 'directors': 666,\n",
       " 'journey': 667,\n",
       " 'lives': 668,\n",
       " 'mind': 669,\n",
       " 'places': 670,\n",
       " 'ones': 671,\n",
       " 'investigation': 672,\n",
       " 'main': 673,\n",
       " 'cant': 674,\n",
       " 'luis': 675,\n",
       " 'detective': 676,\n",
       " 'deal': 677,\n",
       " 'leads': 678,\n",
       " 'szifron': 679,\n",
       " 'leave': 680,\n",
       " 'situation': 681,\n",
       " 'd√≠az': 682,\n",
       " 'silverstein': 683,\n",
       " 'help': 684,\n",
       " 'laughing': 685,\n",
       " 'street': 686,\n",
       " 'hes': 687,\n",
       " 'magic': 688,\n",
       " 'de': 689,\n",
       " 'comedy': 690,\n",
       " 'until': 691,\n",
       " '1930s': 692,\n",
       " 'knows': 693,\n",
       " 'simply': 694,\n",
       " 'supporting': 695,\n",
       " 'count': 696,\n",
       " 'friendly': 697,\n",
       " 'superior': 698,\n",
       " 'garden': 699,\n",
       " 'makers': 700,\n",
       " 'sand': 701,\n",
       " 'finally': 702,\n",
       " 'stage': 703,\n",
       " '80s': 704,\n",
       " 'written': 705,\n",
       " 'gone': 706,\n",
       " 'several': 707,\n",
       " 'version': 708,\n",
       " 'stays': 709,\n",
       " 'book': 710,\n",
       " 'proud': 711,\n",
       " 'war': 712,\n",
       " 'earth': 713,\n",
       " 'right': 714,\n",
       " 'longer': 715,\n",
       " '5': 716,\n",
       " 'showed': 717,\n",
       " 'viewer': 718,\n",
       " 'subtle': 719,\n",
       " 'relationship': 720,\n",
       " 'next': 721,\n",
       " 'expect': 722,\n",
       " 'fact': 723,\n",
       " 'shooting': 724,\n",
       " 'adds': 725,\n",
       " 'adult': 726,\n",
       " 'earlier': 727,\n",
       " 'bugs': 728,\n",
       " 'happy': 729,\n",
       " 'short': 730,\n",
       " 'accidentally': 731,\n",
       " 'often': 732,\n",
       " 'tries': 733,\n",
       " 'jerry': 734,\n",
       " 'ideas': 735,\n",
       " 'within': 736,\n",
       " 'cartoon': 737,\n",
       " 'gentle': 738,\n",
       " 'moment': 739,\n",
       " 'gem': 740,\n",
       " 'late': 741,\n",
       " '60s': 742,\n",
       " 'important': 743,\n",
       " 'changes': 744,\n",
       " 'cruise': 745,\n",
       " 'type': 746,\n",
       " 'soundtrack': 747,\n",
       " 'events': 748,\n",
       " 'king': 749,\n",
       " '1010': 750,\n",
       " 'feature': 751,\n",
       " 'audience': 752,\n",
       " 'fairly': 753,\n",
       " 'normal': 754,\n",
       " 'musical': 755,\n",
       " 'numbers': 756,\n",
       " 'months': 757,\n",
       " 'lose': 758,\n",
       " 'alienate': 759,\n",
       " 'york': 760,\n",
       " 'city': 761,\n",
       " 'society': 762,\n",
       " 'sidney': 763,\n",
       " 'attention': 764,\n",
       " 'kirsten': 765,\n",
       " 'dunst': 766,\n",
       " 'power': 767,\n",
       " 'saving': 768,\n",
       " 'simon': 769,\n",
       " 'pegg': 770,\n",
       " 'sydney': 771,\n",
       " 'later': 772,\n",
       " 'hilarious': 773,\n",
       " 'jeff': 774,\n",
       " 'bridges': 775,\n",
       " 'clayton': 776,\n",
       " 'level': 777,\n",
       " 'charm': 778,\n",
       " 'either': 779,\n",
       " 'memorable': 780,\n",
       " 'elements': 781,\n",
       " 'need': 782,\n",
       " 'head': 783,\n",
       " 'cartoons': 784,\n",
       " 'used': 785,\n",
       " 'involving': 786,\n",
       " 'pictures': 787,\n",
       " 'filled': 788,\n",
       " 'mood': 789,\n",
       " 'wouldnt': 790,\n",
       " 'difficult': 791,\n",
       " 'pace': 792,\n",
       " 'wrong': 793,\n",
       " 'mean': 794,\n",
       " 'wasnt': 795,\n",
       " 'disney': 796,\n",
       " 'original': 797,\n",
       " 'jones': 798,\n",
       " 'moves': 799,\n",
       " 'girl': 800,\n",
       " 'meets': 801,\n",
       " 'matt': 802,\n",
       " 'kill': 803,\n",
       " 'hollywood': 804,\n",
       " 'youre': 805,\n",
       " 'taking': 806,\n",
       " 'wonderfully': 807,\n",
       " 'track': 808,\n",
       " 'youll': 809,\n",
       " 'bar': 810,\n",
       " 'hotel': 811,\n",
       " 'opportunity': 812,\n",
       " 'coming': 813,\n",
       " 'study': 814,\n",
       " 'accepts': 815,\n",
       " 'drinking': 816,\n",
       " 'etc': 817,\n",
       " 'wont': 818,\n",
       " 'stop': 819,\n",
       " 'fire': 820,\n",
       " 'including': 821,\n",
       " 'revenge': 822,\n",
       " 'intelligent': 823,\n",
       " '4': 824,\n",
       " 'drama': 825,\n",
       " 'awards': 826,\n",
       " '1': 827,\n",
       " 'jodelle': 828,\n",
       " 'ferland': 829,\n",
       " 'glad': 830,\n",
       " 'caught': 831,\n",
       " 'turn': 832,\n",
       " 'moviebr': 833,\n",
       " 'script': 834,\n",
       " 'definitely': 835,\n",
       " 'gambling': 836,\n",
       " 'video': 837,\n",
       " 'store': 838,\n",
       " 'huge': 839,\n",
       " 'heard': 840,\n",
       " 'impressive': 841,\n",
       " 'group': 842,\n",
       " 'list': 843,\n",
       " 'l': 844,\n",
       " 'horrible': 845,\n",
       " 'screen': 846,\n",
       " 'production': 847,\n",
       " 'somewhat': 848,\n",
       " 'tale': 849,\n",
       " 'less': 850,\n",
       " 'kidnapped': 851,\n",
       " 'directed': 852,\n",
       " 'fine': 853,\n",
       " 'narrative': 854,\n",
       " 'provides': 855,\n",
       " 'martin': 856,\n",
       " 'richard': 857,\n",
       " 'steps': 858,\n",
       " 'affection': 859,\n",
       " 'psychiatrist': 860,\n",
       " 'steckert': 861,\n",
       " 'telling': 862,\n",
       " 'justin': 863,\n",
       " 'score': 864,\n",
       " 'topnotch': 865,\n",
       " 'picture': 866,\n",
       " 'change': 867,\n",
       " 'filmed': 868,\n",
       " 'giving': 869,\n",
       " '9': 870,\n",
       " 'everyday': 871,\n",
       " 'regular': 872,\n",
       " 'rare': 873,\n",
       " 'forever': 874,\n",
       " 'ended': 875,\n",
       " 'episode': 876,\n",
       " 'line': 877,\n",
       " 'jack': 878,\n",
       " 'eye': 879,\n",
       " 'blockbuster': 880,\n",
       " 'entire': 881,\n",
       " 'imagine': 882,\n",
       " 'week': 883,\n",
       " 'rest': 884,\n",
       " 'left': 885,\n",
       " 'forward': 886,\n",
       " 'fantastic': 887,\n",
       " 'simple': 888,\n",
       " 'century': 889,\n",
       " 'alien': 890,\n",
       " 'manages': 891,\n",
       " 'awful': 892,\n",
       " 'spock': 893,\n",
       " 'decides': 894,\n",
       " 'escape': 895,\n",
       " 'poor': 896,\n",
       " 'cavegirl': 897,\n",
       " 'frozen': 898,\n",
       " 'greatest': 899,\n",
       " 'total': 900,\n",
       " 'fifty': 901,\n",
       " 'doubt': 902,\n",
       " 'understand': 903,\n",
       " 'plan': 904,\n",
       " 'british': 905,\n",
       " 'amusing': 906,\n",
       " 'run': 907,\n",
       " 'mostly': 908,\n",
       " 'odd': 909,\n",
       " 'daily': 910,\n",
       " 'various': 911,\n",
       " 'clever': 912,\n",
       " 'carell': 913,\n",
       " 'sort': 914,\n",
       " 'andy': 915,\n",
       " 'reveals': 916,\n",
       " 'david': 917,\n",
       " 'clear': 918,\n",
       " 'certain': 919,\n",
       " 'respect': 920,\n",
       " 'innocent': 921,\n",
       " 'humor': 922,\n",
       " 'finds': 923,\n",
       " 'throughout': 924,\n",
       " 'addition': 925,\n",
       " 'parts': 926,\n",
       " 'relationships': 927,\n",
       " 'trying': 928,\n",
       " 'enough': 929,\n",
       " 'summer': 930,\n",
       " 'virgin': 931,\n",
       " 'particularly': 932,\n",
       " 'light': 933,\n",
       " 'smiling': 934,\n",
       " 'french': 935,\n",
       " 'women': 936,\n",
       " 'add': 937,\n",
       " 'straight': 938,\n",
       " 'subject': 939,\n",
       " 'beast': 940,\n",
       " 'view': 941,\n",
       " 'mark': 942,\n",
       " 'ways': 943,\n",
       " 'force': 944,\n",
       " 'reason': 945,\n",
       " 'explores': 946,\n",
       " 'sequence': 947,\n",
       " 'desire': 948,\n",
       " 'lets': 949,\n",
       " 'control': 950,\n",
       " 'shocking': 951,\n",
       " 'sex': 952,\n",
       " 'dealt': 953,\n",
       " 'borowczyk': 954,\n",
       " 'dialogue': 955,\n",
       " 'along': 956,\n",
       " 'ground': 957,\n",
       " 'exist': 958,\n",
       " 'taboo': 959,\n",
       " 'portrayed': 960,\n",
       " 'shock': 961,\n",
       " 'missed': 962,\n",
       " 'season': 963,\n",
       " 'fox': 964,\n",
       " 'wait': 965,\n",
       " 'da': 966,\n",
       " 'decided': 967,\n",
       " 'honest': 968,\n",
       " 'gripe': 969,\n",
       " 'atmosphere': 970,\n",
       " 'thomas': 971,\n",
       " 'turns': 972,\n",
       " 'details': 973,\n",
       " 'celebrities': 974,\n",
       " 'doing': 975,\n",
       " 'reminds': 976,\n",
       " 'looks': 977,\n",
       " 'difference': 978,\n",
       " 'henry': 979,\n",
       " 'wise': 980,\n",
       " 'dreams': 981,\n",
       " 'needed': 982,\n",
       " 'care': 983,\n",
       " 'spent': 984,\n",
       " 'stand': 985,\n",
       " 'place': 986,\n",
       " 'whose': 987,\n",
       " 'entertainment': 988,\n",
       " 'four': 989,\n",
       " 'response': 990,\n",
       " 'killer': 991,\n",
       " 'itself': 992,\n",
       " 'parody': 993,\n",
       " 'call': 994,\n",
       " 'comment': 995,\n",
       " 'ago': 996,\n",
       " 'hopefully': 997,\n",
       " 'works': 998,\n",
       " 'complaining': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43773"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IMDBDataset(data_dir= DATA_DIR, tokenizer= nlp, train=True)\n",
    "test_dataset = IMDBDataset(data_dir= DATA_DIR, tokenizer= nlp, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1,    40,  1213,    30, 40822,  1965,   509,   589,     4,   570,\n",
       "          567,   322,    35,     8,  2557, 35167,  2242,    29,   134,    28,\n",
       "          381,    25,   235,  8245,     3,    35,     8,   177,  1105,    97,\n",
       "            9,     8,     3,    54,    65,  6067,    16,  1299,     9,   932,\n",
       "         2972,  2071,    85,   144,  1257,    20,    19,   581,     8,  1802,\n",
       "          752,     6,   107,    88,    35,     8,    95,    16,    85, 13685,\n",
       "          642,    20,  9787,   111,  1300,    16,    65,     5,   599, 10223,\n",
       "           55,    72,    28,     3,    25,  7904,   199,   105,   474,    58,\n",
       "          125,  1023,    15,     8,   214,   509, 17945,     8,   247,    74,\n",
       "           61,   130,    81,    37,   138,   918,    11,    25, 10067,    15,\n",
       "          188,    19,   646,    16,    28,  1084,    17,    47,   192,     4,\n",
       "         2034,  7813,   445,   101,   242,    97,   361,  6777,   169,    31,\n",
       "          938, 26959,  4948,  3540,    20,   977,    46,    16,  6809,  9379,\n",
       "         1694,   171,  5454,    35,     8,     3,    15,   339,  2769,  6007,\n",
       "        15281,     3,    51,    11,  9514,   119,  8779,    35,     8,   729,\n",
       "         4384,    15,  1983,     9,   509,    30,     8, 19958,    16,   157,\n",
       "          929,  1338,    17,   571,   435,    34,   417,    15,     8,   777,\n",
       "           15, 13685,     3,   130,   454,    11,   130,  1425,     4,  1310,\n",
       "          231,    35,     4,    95,    44,  2496,    74,   110,  1874,  4065,\n",
       "           16,    37,   116,   218,    39,  6171,     6, 11831, 14391,   546,\n",
       "            2])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200])\n",
      "tensor([False])\n"
     ]
    }
   ],
   "source": [
    "data_, label_ = next(iter(test_dataset))\n",
    "print(data_.size())\n",
    "print(label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def pad_collate(x):\n",
    "    data = [item[0] for item in x]\n",
    "    label = [int(item[1]) for item in x]\n",
    "    return pad_sequence(data), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_dataset, batch_size=32, collate_fn= pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([610, 32])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dl_, label_dl_ = next(iter(train_dl))\n",
    "data_dl_.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43773"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([610, 32, 256])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = nn.Embedding(num_embeddings=len(nlp.stoi) ,embedding_dim= 256, padding_idx=0)\n",
    "embedding(data_dl_).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=256, hidden_dim=512, output_dim=, num_layers=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers= num_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):            # [seq_len, bs]\n",
    "        x = self.embedding(x)  # [seq_len, bs, 256]\n",
    "        x = self.lstm(x)       # [seq_len, bs, 512]\n",
    "        x = self.fc(x)         # [seq_len, ] \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
